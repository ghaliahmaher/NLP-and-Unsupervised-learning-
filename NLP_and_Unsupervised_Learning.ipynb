{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AzYwUiDkBfVT"
   },
   "source": [
    "#NLP and Unsupervised Learning\n",
    "\n",
    "\n",
    "Done by:\n",
    "\n",
    "\n",
    "1.   Mohammad Alajmi\n",
    "2.   Mohammad Almalki\n",
    "\n",
    "1.   Yahya Alyubi\n",
    "\n",
    "1.   Ghadeer Alghamdi\n",
    "\n",
    "1.   Jamila Alharbi\n",
    "2.   Ghliah maher \n",
    "\n",
    "it is better on google colab\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kR4Jvg68Bn3R"
   },
   "source": [
    "install pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M2Ytt7Rs9Spf",
    "outputId": "59f6e2a8-4767-4e29-9a7b-4e9debba1004"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: camel-tools in c:\\users\\ghali\\anaconda3\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from camel-tools) (2.26.0)\n",
      "Requirement already satisfied: six in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from camel-tools) (1.16.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from camel-tools) (1.7.1)\n",
      "Requirement already satisfied: dill in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from camel-tools) (0.3.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from camel-tools) (1.0.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from camel-tools) (1.20.3)\n",
      "Requirement already satisfied: future in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from camel-tools) (0.18.2)\n",
      "Requirement already satisfied: transformers>=3.0.2 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from camel-tools) (4.14.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from camel-tools) (1.3.4)\n",
      "Requirement already satisfied: torch>=1.3 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from camel-tools) (1.10.1)\n",
      "Requirement already satisfied: docopt in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from camel-tools) (0.6.2)\n",
      "Requirement already satisfied: cachetools in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from camel-tools) (4.2.4)\n",
      "Requirement already satisfied: editdistance in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from camel-tools) (0.6.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from torch>=1.3->camel-tools) (3.10.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from transformers>=3.0.2->camel-tools) (21.3)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from transformers>=3.0.2->camel-tools) (0.0.46)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from transformers>=3.0.2->camel-tools) (4.62.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from transformers>=3.0.2->camel-tools) (0.2.1)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from transformers>=3.0.2->camel-tools) (0.10.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from transformers>=3.0.2->camel-tools) (2021.8.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from transformers>=3.0.2->camel-tools) (5.4.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from transformers>=3.0.2->camel-tools) (3.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers>=3.0.2->camel-tools) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers>=3.0.2->camel-tools) (0.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from pandas->camel-tools) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from pandas->camel-tools) (2021.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from requests->camel-tools) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from requests->camel-tools) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from requests->camel-tools) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from requests->camel-tools) (2.0.4)\n",
      "Requirement already satisfied: click in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from sacremoses->transformers>=3.0.2->camel-tools) (8.0.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from sacremoses->transformers>=3.0.2->camel-tools) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from scikit-learn->camel-tools) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install camel-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UaV3tPnsBnl7",
    "outputId": "dffea07d-f889-4552-fd3a-e30c1227d1aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: farasapy in c:\\users\\ghali\\anaconda3\\lib\\site-packages (0.0.14)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from farasapy) (4.62.3)\n",
      "Requirement already satisfied: requests in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from farasapy) (2.26.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from requests->farasapy) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from requests->farasapy) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from requests->farasapy) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from requests->farasapy) (2.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from tqdm->farasapy) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip  install  farasapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "SNa-KtKDRnus"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bertopic[all] in c:\\users\\ghali\\anaconda3\\lib\\site-packages (0.9.4)\n",
      "Requirement already satisfied: sentence-transformers>=0.4.1 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from bertopic[all]) (2.1.0)\n",
      "Requirement already satisfied: hdbscan>=0.8.27 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from bertopic[all]) (0.8.27)\n",
      "Requirement already satisfied: umap-learn>=0.5.0 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from bertopic[all]) (0.5.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from bertopic[all]) (1.20.3)\n",
      "Requirement already satisfied: pyyaml<6.0 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from bertopic[all]) (5.4.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2.post1 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from bertopic[all]) (1.0.1)\n",
      "Requirement already satisfied: pandas>=1.1.5 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from bertopic[all]) (1.3.4)\n",
      "Requirement already satisfied: plotly>=4.7.0 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from bertopic[all]) (5.5.0)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from bertopic[all]) (4.62.3)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from hdbscan>=0.8.27->bertopic[all]) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from hdbscan>=0.8.27->bertopic[all]) (1.7.1)\n",
      "Requirement already satisfied: cython>=0.27 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from hdbscan>=0.8.27->bertopic[all]) (0.29.23)\n",
      "Requirement already satisfied: six in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from hdbscan>=0.8.27->bertopic[all]) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from pandas>=1.1.5->bertopic[all]) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from pandas>=1.1.5->bertopic[all]) (2.8.2)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from plotly>=4.7.0->bertopic[all]) (8.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.2.post1->bertopic[all]) (2.2.0)\n",
      "Requirement already satisfied: tokenizers>=0.10.3 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic[all]) (0.10.3)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic[all]) (0.2.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic[all]) (0.11.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic[all]) (3.6.6)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic[all]) (4.14.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic[all]) (1.10.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic[all]) (0.1.95)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic[all]) (3.10.0.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from tqdm>=4.41.1->bertopic[all]) (0.4.4)\n",
      "Requirement already satisfied: requests in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic[all]) (2.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic[all]) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic[all]) (2021.8.3)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic[all]) (0.0.46)\n",
      "Requirement already satisfied: filelock in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic[all]) (3.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic[all]) (3.0.4)\n",
      "Requirement already satisfied: pynndescent>=0.5 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from umap-learn>=0.5.0->bertopic[all]) (0.5.5)\n",
      "Requirement already satisfied: numba>=0.49 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from umap-learn>=0.5.0->bertopic[all]) (0.54.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic[all]) (58.0.4)\n",
      "Requirement already satisfied: llvmlite<0.38,>=0.37.0rc1 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic[all]) (0.37.0)\n",
      "Requirement already satisfied: click in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers>=0.4.1->bertopic[all]) (8.0.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic[all]) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic[all]) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic[all]) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic[all]) (2021.10.8)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from torchvision->sentence-transformers>=0.4.1->bertopic[all]) (8.4.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: bertopic 0.9.4 does not provide the extra 'all'\n"
     ]
    }
   ],
   "source": [
    "!pip install bertopic[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "psvVfgPf3uWT",
    "outputId": "73aa3db6-e574-4cf5-daf1-3d4a879aeb0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flair in c:\\users\\ghali\\anaconda3\\lib\\site-packages (0.10)\n",
      "Requirement already satisfied: more-itertools~=8.8.0 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from flair) (8.8.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from flair) (4.7.1)\n",
      "Requirement already satisfied: conllu>=4.0 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from flair) (4.4.1)\n",
      "Requirement already satisfied: deprecated>=1.2.4 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from flair) (1.2.13)\n",
      "Requirement already satisfied: wikipedia-api in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from flair) (0.5.4)\n",
      "Requirement already satisfied: segtok>=1.5.7 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from flair) (1.5.11)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from flair) (0.2.1)\n",
      "Requirement already satisfied: bpemb>=0.3.2 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from flair) (0.3.3)\n",
      "Requirement already satisfied: tabulate in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from flair) (0.8.9)\n",
      "Requirement already satisfied: mpld3==0.3 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from flair) (0.3)\n",
      "Requirement already satisfied: langdetect in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from flair) (1.0.9)\n",
      "Requirement already satisfied: gensim>=3.4.0 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from flair) (4.1.2)\n",
      "Requirement already satisfied: sqlitedict>=1.6.0 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from flair) (1.7.0)\n",
      "Requirement already satisfied: janome in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from flair) (0.4.1)\n",
      "Requirement already satisfied: sentencepiece==0.1.95 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from flair) (0.1.95)\n",
      "Requirement already satisfied: gdown==3.12.2 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from flair) (3.12.2)\n",
      "Requirement already satisfied: regex in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from flair) (2021.8.3)\n",
      "Requirement already satisfied: torch!=1.8,>=1.5.0 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from flair) (1.10.1)\n",
      "Requirement already satisfied: tqdm>=4.26.0 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from flair) (4.62.3)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from flair) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from flair) (2.8.2)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from flair) (3.5.0)\n",
      "Requirement already satisfied: ftfy in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from flair) (6.0.3)\n",
      "Requirement already satisfied: transformers>=4.0.0 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from flair) (4.14.1)\n",
      "Requirement already satisfied: konoha<5.0.0,>=4.0.0 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from flair) (4.6.5)\n",
      "Requirement already satisfied: six in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from gdown==3.12.2->flair) (1.16.0)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from gdown==3.12.2->flair) (2.26.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from gdown==3.12.2->flair) (3.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from bpemb>=0.3.2->flair) (1.20.3)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from deprecated>=1.2.4->flair) (1.12.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from gensim>=3.4.0->flair) (5.2.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from gensim>=3.4.0->flair) (1.7.1)\n",
      "Requirement already satisfied: Cython==0.29.23 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from gensim>=3.4.0->flair) (0.29.23)\n",
      "Requirement already satisfied: importlib-metadata<4.0.0,>=3.7.0 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from konoha<5.0.0,>=4.0.0->flair) (3.10.1)\n",
      "Requirement already satisfied: overrides<4.0.0,>=3.0.0 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from konoha<5.0.0,>=4.0.0->flair) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (4.25.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (8.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (3.0.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (0.11.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from requests[socks]->gdown==3.12.2->flair) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from requests[socks]->gdown==3.12.2->flair) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from requests[socks]->gdown==3.12.2->flair) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from requests[socks]->gdown==3.12.2->flair) (3.3)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from scikit-learn>=0.21.3->flair) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from scikit-learn>=0.21.3->flair) (2.2.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from torch!=1.8,>=1.5.0->flair) (3.10.0.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from tqdm>=4.26.0->flair) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from transformers>=4.0.0->flair) (0.10.3)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from transformers>=4.0.0->flair) (0.0.46)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from transformers>=4.0.0->flair) (5.4.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from ftfy->flair) (0.2.5)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from requests[socks]->gdown==3.12.2->flair) (1.7.1)\n",
      "Requirement already satisfied: click in c:\\users\\ghali\\anaconda3\\lib\\site-packages (from sacremoses->transformers>=4.0.0->flair) (8.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user flair\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s7JI6AvHB6Pt"
   },
   "source": [
    "import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "eGjZitDgGKup"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import LdaMulticore\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from camel_tools.utils.normalize import normalize_alef_ar\n",
    "from camel_tools.utils.normalize import normalize_teh_marbuta_ar\n",
    "from camel_tools.utils.normalize import normalize_alef_maksura_ar\n",
    "import unicodedata as ud\n",
    "\n",
    "\n",
    "import farasa\n",
    "from farasa import stemmer\n",
    "from farasa.pos import FarasaPOSTagger\n",
    "from farasa.ner import FarasaNamedEntityRecognizer\n",
    "from farasa.diacratizer import FarasaDiacritizer\n",
    "from farasa.segmenter import FarasaSegmenter\n",
    "from farasa.stemmer import FarasaStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZExreG58Bhf9",
    "outputId": "e306f11e-81b5-4a10-83ce-afe7486e0cae"
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# # nltk.download('stopwords')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pyd1PrqeHI5O"
   },
   "source": [
    "# Load Data\n",
    "the data was generated by the developing team using web scrapping and google foems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hxs5jgSq9XUS",
    "outputId": "a418f6c6-f238-400d-8834-844029448001"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "D4D7RY36GF0L",
    "outputId": "546cf3af-4790-49b8-c3fa-a2ea88034d30",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# data=  pd.read_excel (\"احتمالات الشكاوى.xlsx\")\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "v5Znx7MIf4UX"
   },
   "outputs": [],
   "source": [
    "# data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FV3SAzGmOSjf"
   },
   "source": [
    "#Preprocessing\n",
    "\n",
    "\n",
    "\n",
    "1.   replace some words\n",
    "2.   remove stopwords\n",
    "\n",
    "1.   normlize (ا,ى,ه)\n",
    "\n",
    "1.   stemming\n",
    "2.   more replacing with the correct word\n",
    "\n",
    "2.   part of speach \n",
    "\n",
    "1.   count vectorizor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "ZcBjVmIpEVJN",
    "outputId": "652479e7-072a-477a-f0cb-eaa2dd72bdca"
   },
   "outputs": [],
   "source": [
    "# data['text'] = data['text'].str.replace('ورائي','خلفي')\n",
    "# data['text'] = data['text'].str.replace('وراي','خلفي')\n",
    "# data['text'] = data['text'].str.replace('ورا','خلف')\n",
    "# data['text'] = data['text'].str.replace('ورايا','خلف')\n",
    "\n",
    "# data['text'] = data['text'].str.replace('موتر','سيارة')\n",
    "# data['text'] = data['text'].str.replace('موتري','سيارة')\n",
    "# data['text'] = data['text'].str.replace('السياره','سيارة')\n",
    "# data['text'] = data['text'].str.replace('سياره','سيارة')\n",
    "# data['text'] = data['text'].str.replace('سيارتي','سيارة')\n",
    "# data['text'] = data['text'].str.replace('سيارتى','سيارة')\n",
    "\n",
    "# data['text'] = data['text'].str.replace('سكرت','قفل')\n",
    "# data['text'] = data['text'].str.replace('سكر','قفل')\n",
    "# data['text'] = data['text'].str.replace('صاكه','قفل')\n",
    "# data['text'] = data['text'].str.replace('انعدمت','خدشت')\n",
    "\n",
    "# data['text'] = data['text'].str.replace('ورع','طفل')\n",
    "# data['text'] = data['text'].str.replace('بيبي','طفل')\n",
    "\n",
    "# data['text'] = data['text'].str.replace('قسائمي','مخالفاتي')\n",
    "\n",
    "\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jBrfeiL69ovx",
    "outputId": "c0e04532-a6ae-4803-dd32-391ce039083d"
   },
   "outputs": [],
   "source": [
    "# print(data['text'])\n",
    "# arb_stopwords = ['الحل','مانوع','ارى','من','وين','معرف','مطفأة','طارئة',' الحالة','جدا','مهم','طواريء','بلاغ','استطع','لم','لان','ولا','الفاعل','الشكوى','اريد','علي','عن','صار','ابغى','ايش','بها','هي','هو','على','وش','ان','لي','هل','فيه','فيها','ماهي','هي','ما','في','هناك','واودّ','و','تم','قام','عليه','عليهم','لقيت','الظاهر']\n",
    "\n",
    "# print(arb_stopwords)\n",
    "# data['text']= data.apply(lambda row: normalize_alef_ar(row['text']), axis=1)\n",
    "# data['text']= data.apply(lambda row: normalize_alef_maksura_ar(row['text']), axis=1)\n",
    "# data['text'] = data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (arb_stopwords)]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# for index, row in data.iterrows():\n",
    "#     row['text']=''.join(c for c in row['text'] if not ud.category(c).startswith('P'))\n",
    "\n",
    "#     print( row['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Z1BQFknr_Bdh"
   },
   "outputs": [],
   "source": [
    "# stemmer = FarasaStemmer()\n",
    "# stem_arr=[]\n",
    "# for index, row in data.iterrows(): \n",
    "#     stem_arr.append(stemmer.stem(row['text']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "79a-0RvlC0un"
   },
   "outputs": [],
   "source": [
    "# for i in range(0, len(stem_arr)):\n",
    "#     print(stem_arr[i])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stem_arr_file = open(\"stem_arr_file.txt\", \"w\",encoding=\"utf8\")\n",
    "# for i in range(0, len(stem_arr)): \n",
    "#     stem_arr_file.write(stem_arr[i] + \"\\n\")\n",
    "# stem_arr_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uiJha_uKNavX",
    "outputId": "41093e26-2f9e-4e4a-ae4f-8ef9526b5e02"
   },
   "outputs": [],
   "source": [
    "# stem_arr=[]\n",
    "# f = open(\"stem_arr_file.txt\", \"r\",encoding=\"utf8\")\n",
    "# stem_arr = f.readlines()\n",
    "# print(stem_arr)\n",
    "\n",
    "# for i in range(0, len(stem_arr)): \n",
    "#     stem_arr[i]=stem_arr[i].strip()\n",
    "# print(stem_arr)\n",
    "\n",
    "# f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "YnHtmUxbSu1j"
   },
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"CAMeL-Lab/bert-base-arabic-camelbert-mix-pos-glf\")\n",
    "# model = AutoModelForTokenClassification.from_pretrained(\"CAMeL-Lab/bert-base-arabic-camelbert-mix-pos-glf\")\n",
    "# pos = pipeline('token-classification', model='CAMeL-Lab/bert-base-arabic-camelbert-mix-pos-glf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "1SqeeQvcfm6C"
   },
   "outputs": [],
   "source": [
    "# pos_arr=[]\n",
    "# for i  in range(0, len(stem_arr)): \n",
    "#    pos_arr.append(pos(stem_arr[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "NuTxKOYMa3Me"
   },
   "outputs": [],
   "source": [
    "\n",
    "# for i  in range(0, len(pos_arr)): \n",
    "#   print(pos_arr[i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 667
    },
    "id": "0tWlFWNVD_-o",
    "outputId": "f96b6463-aa1d-4da8-a1eb-cfc9e39be05b"
   },
   "outputs": [],
   "source": [
    "# df = pd.DataFrame((stem_arr), columns =['text_token'])\n",
    "# print(df)\n",
    "# result = pd.concat([data, df], axis=1, join='inner')\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "jADr2pJllUO-",
    "outputId": "b81de7e2-d5eb-44e9-ff34-62387846ad04"
   },
   "outputs": [],
   "source": [
    "# result['text_token'] = result['text_token'].apply(lambda x: x.split())\n",
    "\n",
    "# result['text_token']=result['text_token'].apply(lambda x: [item for item in x if item not in arb_stopwords])\n",
    "# result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "qPYAbNmTvQnI"
   },
   "outputs": [],
   "source": [
    "# count_vectorizer = CountVectorizer(stop_words=arb_stopwords)\n",
    "# matrix = count_vectorizer.fit_transform(result.text_token)\n",
    "\n",
    "# counts = pd.DataFrame(matrix.toarray(),columns=count_vectorizer.get_feature_names())\n",
    "# counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "1S6Q96ZmGZhr"
   },
   "outputs": [],
   "source": [
    "# documents = result['text_token'].values\n",
    "# print(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2SRCMaPIizW"
   },
   "source": [
    "#Embedding model\n",
    "the model in this project is (bert-base-arabertv02) from [Hugging Face](https://huggingface.co/models)\n",
    "\n",
    "For more deatelis check out BERTopic decomntion [here](https://maartengr.github.io/BERTopic/tutorial/embeddings/embeddings.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "XCW-jMbyUffN"
   },
   "outputs": [],
   "source": [
    "#to experiment with other BERT models simply change the model name below\n",
    "# arabert = TransformerDocumentEmbeddings('aubmindlab/bert-base-arabertv02')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBcNmZJzSTY8"
   },
   "source": [
    "# **Create Topics**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkbmf2rmhGZx"
   },
   "source": [
    "the default value of the bertopic was 24. however, due to the small size of the data, we specified the number of topics to be 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "OGiQB1iC8pFl"
   },
   "outputs": [],
   "source": [
    "# topic_model = BERTopic(language=\"arabic\", low_memory=True ,calculate_probabilities=False,embedding_model=arabert,nr_topics=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "TfhfzqkoSJ1I"
   },
   "outputs": [],
   "source": [
    "# topics, probs = topic_model.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "AiqknMl5yfqo"
   },
   "outputs": [],
   "source": [
    "# topic_model.get_topic_freq()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "krD3o2zufHoV"
   },
   "source": [
    "-1 refers to all outliers and should typically be ignored. Next, let's take a look at the most frequent topic that was generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "KRgQp9IT7rnX"
   },
   "outputs": [],
   "source": [
    "# topic_model.get_topic(-1)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "lkwKLYvw7rgl"
   },
   "outputs": [],
   "source": [
    "# topic_model.get_topic(0)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "05Mu184PhuLK"
   },
   "outputs": [],
   "source": [
    "# topic_model.get_topic(1)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "z23uQ3w4zRxC",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# topic_model.get_topic(2)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "XRwJ65LdzSgW",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# topic_model.get_topic(3)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "EU6Br53VL8UY"
   },
   "outputs": [],
   "source": [
    "# topic_model.get_topic(4)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "VL2QOqBDW7bm"
   },
   "outputs": [],
   "source": [
    "# topic_model.get_topic(5)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "HxyDbIv3f4DK"
   },
   "outputs": [],
   "source": [
    "# topic_model.get_topic(6)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_model.get_topic(7)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1UCb_fnUo_rJ"
   },
   "source": [
    "# LDA\n",
    "\n",
    "We use the [ parallelized Latent Dirichlet Allocation (LDA)](https://radimrehurek.com/gensim/models/ldamulticore.html) from Gensim.\n",
    "\n",
    "Note: for LDA you have to define topics number in advance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "RGlCtdo4bY21"
   },
   "outputs": [],
   "source": [
    "# texts = [[word for word in str(document).split()] for document in documents]\n",
    "# id2word = corpora.Dictionary(texts)\n",
    "# corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "FP89p9qGpA86"
   },
   "outputs": [],
   "source": [
    "# no_topics = 7\n",
    "# lda = LdaMulticore(corpus, id2word=id2word, num_topics=no_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l6gPvQtyaUOf",
    "outputId": "ba12e5bb-e274-4d46-cb15-c3c97c081f67"
   },
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "# pprint(lda.print_topics())\n",
    "# doc_lda = lda[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "ep0Pl3qqpevJ"
   },
   "outputs": [],
   "source": [
    "# coherence_model_lda = CoherenceModel(model=lda, texts=texts, dictionary=id2word, coherence='c_v')\n",
    "# coherence_lda = coherence_model_lda.get_coherence()\n",
    "# print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rD-h3cbRdG9p",
    "outputId": "4bc287e1-c81f-437e-d36c-aa2306b7596c"
   },
   "outputs": [],
   "source": [
    "# lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "sX36sCI-yfhH"
   },
   "outputs": [],
   "source": [
    "# !pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "3SeKHouJdG9q"
   },
   "outputs": [],
   "source": [
    "# import pyLDAvis\n",
    "# import gensim\n",
    "# import pyLDAvis\n",
    "# import pyLDAvis.gensim\n",
    "# vis = gensim.prepare(lda, corpus, dictionary=lda_model.id2word)\n",
    "# vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAbf7WWupz6B"
   },
   "source": [
    "#NMF\n",
    "We use Scikit-learn implementation of [NMF](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html).\n",
    "\n",
    "Note: for NMF you have to define topics number in advance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ongX2jbZpifi",
    "outputId": "c82f132d-c46f-463b-93b3-8873b2f3939a"
   },
   "outputs": [],
   "source": [
    "# tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2)\n",
    "# tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "# tfidf_feature_names = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CE7N4UPUqlpf",
    "outputId": "9e61fe5f-b3de-442b-d59c-20c361ec01c8"
   },
   "outputs": [],
   "source": [
    "# no_topics = 7\n",
    "# nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j9Fm3Ox1q1WU",
    "outputId": "83b51f4f-0060-4963-8c33-d4a365e842d7"
   },
   "outputs": [],
   "source": [
    "# topics_NMF=[]\n",
    "# for index, topic in enumerate(nmf.components_):\n",
    "#     row=[]\n",
    "#     for i in topic.argsort()[-10:]:\n",
    "#       row.append(tfidf_vectorizer.get_feature_names()[i])\n",
    "#     topics_NMF.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ka-9N3RgY-_8",
    "outputId": "d666684c-8acc-43e8-a7b8-d183f1739d02"
   },
   "outputs": [],
   "source": [
    "# for i in range(0,len(topics_NMF)):\n",
    "#   print(topics_NMF[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "BEzvoNFPrBft"
   },
   "outputs": [],
   "source": [
    "# cm = CoherenceModel(topics=topics_NMF, texts=texts, corpus=corpus, dictionary=id2word, coherence='c_npmi')\n",
    "# coherence_nmf = cm.get_coherence()  \n",
    "# print('\\nCoherence Score: ', coherence_nmf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edo1iBWnOKJl"
   },
   "source": [
    "#Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "XxHtjF3q7Yo2"
   },
   "outputs": [],
   "source": [
    "# texts = [[word for word in str(document).split()] for document in documents]\n",
    "# print(texts)\n",
    "# id2word = corpora.Dictionary(texts)\n",
    "# print(id2word)\n",
    "# corpus = [id2word.doc2bow(text) for text in texts]\n",
    "# print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "PHbifeYYv7rx"
   },
   "outputs": [],
   "source": [
    "# topics=[]\n",
    "# for i in topic_model.get_topics():\n",
    "#   row=[]\n",
    "#   topic= topic_model.get_topic(i)\n",
    "#   for word in topic:\n",
    "#      row.append(word[0])\n",
    "#   topics.append(row)\n",
    "# print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "onqm-4funXW5"
   },
   "outputs": [],
   "source": [
    "# compute Coherence Score\n",
    "\n",
    "# cm = CoherenceModel(model=topic_model,topics=topics, texts=texts, corpus=corpus, dictionary=id2word, coherence='c_npmi')\n",
    "# coherence = cm.get_coherence() \n",
    "# print('\\nCoherence Score: ', coherence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8c8LenB8Zyl"
   },
   "source": [
    "# **Visualize Topics**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "AQKBcla28bY0"
   },
   "outputs": [],
   "source": [
    "# topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "qZNuKoVePavN"
   },
   "outputs": [],
   "source": [
    "# topic_model.visualize_barchart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wekNoQNuUVoU"
   },
   "source": [
    "# Model saving\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p saved_model\n",
    "# topic_model.save('saved_model/my_model') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "nWUF1uxiSb_a"
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "# topic_model.save(\"my_model4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QdjtcqszPik5"
   },
   "source": [
    "#Model loding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # my_model directory\n",
    "# !ls saved_model\n",
    "\n",
    "# # Contains an assets folder, saved_model.pb, and variables folder.\n",
    "# !ls saved_model/my_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_model = tf.keras.models.load_model('saved_model/my_model')\n",
    "\n",
    "# # Check its architecture\n",
    "# new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "y_eHBI1jSb6i"
   },
   "outputs": [],
   "source": [
    "# Load model\n",
    "\n",
    "my_model = BERTopic.load(\"saved_model/my_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_user='السلام عليكم'\n",
    "\n",
    "# in_user=in_user.replace('ورائي','خلفي')\n",
    "# in_user=in_user.replace('وراي','خلفي')\n",
    "# in_user=in_user.replace('ورا','خلف')\n",
    "# in_user=in_user.replace('ورايا','خلف')\n",
    "\n",
    "# in_user=in_user.replace('موتر','سيارة')\n",
    "# in_user=in_user.replace('موتري','سيارة')\n",
    "# in_user=in_user.replace('السياره','سيارة')\n",
    "# in_user=in_user.replace('سياره','سيارة')\n",
    "# in_user=in_user.replace('سيارتي','سيارة')\n",
    "# in_user=in_user.replace('سيارتى','سيارة')\n",
    "\n",
    "# in_user=in_user.replace('سكرت','قفل')\n",
    "# in_user=in_user.replace('سكر','قفل')\n",
    "# in_user=in_user.replace('صاكه','قفل')\n",
    "# in_user=in_user.replace('انعدمت','خدشت')\n",
    "\n",
    "# in_user=in_user.replace('ورع','طفل')\n",
    "# in_user=in_user.replace('بزر','طفل')\n",
    "# in_user=in_user.replace('بيبي','طفل')\n",
    "\n",
    "# in_user=in_user.replace('قسائمي','مخالفاتي')\n",
    "\n",
    "# arb_stopwords = ['الحل','مانوع','ارى','من','وين','معرف','مطفأة','طارئة',' الحالة','جدا','مهم','طواريء','بلاغ','استطع','لم','لان','ولا','الفاعل','الشكوى','اريد','علي','عن','صار','ابغى','ايش','بها','هي','هو','على','وش','ان','لي','هل','فيه','فيها','ماهي','هي','ما','في','هناك','واودّ','و','تم','قام','عليه','عليهم','لقيت','الظاهر']\n",
    "# in_user=normalize_alef_ar(in_user)\n",
    "# in_user=normalize_alef_maksura_ar(in_user)\n",
    "# y=in_user.split()\n",
    "# print(y)\n",
    "# n=[]\n",
    "# for i in range(0, len(y)):\n",
    "#     if(y[i] not in arb_stopwords):\n",
    "#         n.append(y[i])\n",
    "# print(y)\n",
    "# print(n)\n",
    "\n",
    "# m=[]\n",
    "\n",
    "# for i in range(0, len(n)):\n",
    "#     m.append(''.join(c for c in n[i] if not ud.category(c).startswith('P')))\n",
    "\n",
    "# x = \" \".join(m)\n",
    "# stemmer=FarasaStemmer()\n",
    "# x=stemmer.stem(x)\n",
    "# print(x)\n",
    "\n",
    "# topics= my_model.transform(x)\n",
    "# print(topics[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "BUqAmwDsuG-u"
   },
   "outputs": [],
   "source": [
    "# my_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "52JL431rDykW"
   },
   "outputs": [],
   "source": [
    "# topics_names=my_model.topic_names\n",
    "# topics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics_names.update({-1:\"طفل محتجز\"})\n",
    "# topics_names.update({0:\"مخالفات\"})\n",
    "# topics_names.update({1:\"اصتدام\"})\n",
    "# topics_names.update({2:\"طريق مقفل\"})\n",
    "# topics_names.update({3:\"طفل محتجز\"})\n",
    "# topics_names.update({4:\"طفل محتجز\"})\n",
    "# topics_names.update({5:\"طريق مقفل\"})\n",
    "# topics_names.update({6:\"اصتدام\"})\n",
    "# topics_names.update({7:\"ترحيب\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics_names2=my_model.topic_names\n",
    "# topics_names2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "2IYxi5tKEPqJ"
   },
   "outputs": [],
   "source": [
    "# my_model.get_topic_freq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "SsNhbJJXuNbI"
   },
   "outputs": [],
   "source": [
    "# topic_children=my_model.get_topic(-1)+my_model.get_topic(3)+my_model.get_topic(4)\n",
    "# topic_children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "Yrj6K6RZruZy"
   },
   "outputs": [],
   "source": [
    "# topic_violation=my_model.get_topic(0)\n",
    "# topic_violation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "I_lxAKF6uM2c"
   },
   "outputs": [],
   "source": [
    "# topic_scratch=my_model.get_topic(1)+my_model.get_topic(6)\n",
    "# topic_scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "6Oui9n35uM6h"
   },
   "outputs": [],
   "source": [
    "# topic_closing_road=my_model.get_topic(2)+my_model.get_topic(5)\n",
    "# topic_closing_road"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_welcome=my_model.get_topic(7)\n",
    "# topic_welcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mGs7p_nQfmm7",
    "outputId": "1a972178-209c-40d7-d7e4-76cf6a0cec12"
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# a_file = open(\"/content/drive/MyDrive/NLP/words.pkl\", \"rb\")\n",
    "# output = pickle.load(a_file)\n",
    "# print(output)\n",
    "\n",
    "# a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "te-PqEQFjFl_",
    "outputId": "4442ce6e-88c6-4587-8deb-08c219278033"
   },
   "outputs": [],
   "source": [
    "# for key, values in output.items():\n",
    "#     print('Key :: ', key)\n",
    "#     if(isinstance(values, list)):\n",
    "#         for value in values:\n",
    "#             print(value)\n",
    "#     else:\n",
    "#         print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "ZgfBsbRAfyO3"
   },
   "outputs": [],
   "source": [
    "\n",
    "# z=''\n",
    "# while True:\n",
    "\n",
    "#z=input()\n",
    "#  if z=='السلام عليكم':\n",
    "#    print(\"Bot -> وعليكم السلام ورحمة بركاته؛ اهلا ... كيف استطيع خدمتك؟\")\n",
    "#  elif z=='مع السلامه':\n",
    "#    print(\"Bot -> مع السلامه\")\n",
    "\n",
    "#    break\n",
    "#  else:\n",
    "#   z=z.replace('وراي','خلفي')\n",
    "#   z=z.replace('ورا','خلف')\n",
    "#   arb_stopwords = ['علي','عن','صار','ابغى','ايش','بها','هي','هو','على','وش','ان','لي','هل','فيه','فيها','ماهي','هي','ما','في','هناك','واودّ','و','تم','قام','عليه','عليهم','لقيت','الظاهر']\n",
    "#   z=normalize_alef_ar(z)\n",
    "#   z=normalize_teh_marbuta_ar(z)\n",
    "#   z=normalize_alef_maksura_ar(z)\n",
    "#   y=z.split()\n",
    "#   x=''\n",
    "#   for i in range(0, len(y)):\n",
    "#     if(y[i] not in arb_stopwords):\n",
    "#       x=''+y[i]\n",
    "\n",
    "#   stemmer = FarasaStemmer()\n",
    "#   x=stemmer.stem(x)\n",
    "#   x=x.replace('سيارةه','سيارة')\n",
    "#   x=x.replace('سيارةة','سيارة')\n",
    "#   x=x.replace('سيار','سيارة')\n",
    "\n",
    "#   z=x.split()\n",
    "#   for i in range(0,len(z)):\n",
    "#     for key,value in output.items():\n",
    "#      if z[i] in value:\n",
    "#        if key == 'مخالفات':\n",
    "#          print('Bot -> الاستفسار عن مخالفه: الرجاء تزويدنا برقم لوحة السيارة')\n",
    "#        elif key == 'طفل محتجز':\n",
    "#          print('Bot -> التبليغ عن اطفال متجزين في سيارة: الرجاء تزويدنا برقم لوحة السيارة')\n",
    "#        elif key  == 'موقف ثنائي':\n",
    "#         print('Bot -> التبليغ عن خدش في السيارة: الرجاء تزويدنا برقم لوحة السيارة')\n",
    "#        elif key  == 'اصتدام':\n",
    "#         print('Bot -> التبليغ عن سيارة متوقفة خلف السيارة: الرجاء تزويدنا برقم لوحة السيارة')\n",
    "#        else:\n",
    "#         print('Bot -> لا استطيع فهمك.\\n الرجاء ادخال الشكوى الصحيحه:\\n 1- استفسار عن مخالفه.\\n 2- تبليغ عن خدش في السياره.\\n 3- اطفال مغلق عليهم في سيارة.\\n 4- سيارة موقفة خلف السيارة.')\n",
    "#     else:\n",
    "#        continue\n",
    "\n",
    " \n",
    "      \n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no display found. Using non-interactive Agg backend\n",
      "['السلام', 'عليكم']\n",
      "['السلام', 'عليكم']\n",
      "['السلام', 'عليكم']\n",
      "سلام على\n",
      "([7], None)\n",
      "['ممكن', 'اعرف', 'ايش', 'هيا', 'مخالفاتي']\n",
      "['ممكن', 'اعرف', 'ايش', 'هيا', 'مخالفاتي']\n",
      "['ممكن', 'اعرف', 'هيا', 'مخالفاتي']\n",
      "ممكن أعرف هيا مخالفة\n",
      "([0], None)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib as mpl\n",
    "if os.environ.get('DISPLAY','') == '':\n",
    "    print('no display found. Using non-interactive Agg backend')\n",
    "    mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import tkinter as tk\n",
    "from tkinter import *\n",
    "import re\n",
    "root = Tk()\n",
    "root.title(\"Chatbot\")\n",
    "\n",
    "def violation():\n",
    "    send = \"You -> \"+e.get()\n",
    "    txt.insert(END, \"\\n\"+send)\n",
    "    o=e.get()\n",
    "    if(re.findall('\\d\\d\\d\\d\\s*',o)):\n",
    "         txt.insert(END, \"\\n\" + \"Bot -> مخالفه موقف ثنائي كانت في الساعة ٦  \\n في موقع الجامعة السعودية الإلكترونية \\n والمبلغ المستحق 500 ريال\\n برقم سداد 789123\")\n",
    "    e.delete(0, 'end')\n",
    "\n",
    "    \n",
    "def alone_child():\n",
    "    send = \"You -> \"+e.get()\n",
    "    txt.insert(END, \"\\n\"+send)\n",
    "    o=e.get()\n",
    "    if(re.findall('\\d\\d\\d\\d\\s*',e.get())):\n",
    "        txt.insert(END, \"\\n\" + \"Bot -> تم رفع البلاغ وسيتم التواصل مع صاحب المركبة \\n شكرا لاهتمامكم\") \n",
    "    e.delete(0, 'end')\n",
    "\n",
    "    \n",
    "def scratch():\n",
    "    send = \"You -> \"+e.get()\n",
    "    txt.insert(END, \"\\n\"+send)\n",
    "    o=e.get()\n",
    "    if(re.findall('\\d\\d\\d\\d\\s*',e.get())):\n",
    "        txt.insert(END, \"\\n\" + \"Bot ->تم رفع البلاغ برقم 123456  سيتم التواصل معكم على الرقم المسجل قريبا \\n شكرا لتواصلكم معنا \")    \n",
    "    e.delete(0, 'end')\n",
    "\n",
    "    \n",
    "def close_road():\n",
    "    send = \"You -> \"+e.get()\n",
    "    txt.insert(END, \"\\n\"+send)\n",
    "    o=e.get()\n",
    "    if(re.findall('\\d\\d\\d\\d\\s*',e.get())):\n",
    "        txt.insert(END, \"\\n\" + \"Bot -> تسيتم التواصل مع صاحب المركبة وإبلاغه بمشكلتكم \\n شكرا لتواصلكم\")                \n",
    "    e.delete(0, 'end')\n",
    "def send():\n",
    "    \n",
    "    send = \"You -> \"+e.get()\n",
    "    txt.insert(END, \"\\n\"+send)\n",
    "    \n",
    "    if(e.get() == \"مع السلامه\"):\n",
    "        txt.insert(END, \"\\n\" + \"Bot -> مع السلامه\")\n",
    "       \n",
    "    else:\n",
    "        if(e.get()==''):\n",
    "            print('888')\n",
    "        else:\n",
    "            in_user=e.get()\n",
    "            in_user=in_user.replace('ورائي','خلفي')\n",
    "            in_user=in_user.replace('وراي','خلفي')\n",
    "            in_user=in_user.replace('ورا','خلف')\n",
    "            in_user=in_user.replace('ورايا','خلف')\n",
    "\n",
    "            in_user=in_user.replace('موتر','سيارة')\n",
    "            in_user=in_user.replace('موتري','سيارة')\n",
    "            in_user=in_user.replace('السياره','سيارة')\n",
    "            in_user=in_user.replace('سياره','سيارة')\n",
    "            in_user=in_user.replace('سيارتي','سيارة')\n",
    "            in_user=in_user.replace('سيارتى','سيارة')\n",
    "\n",
    "            in_user=in_user.replace('سكرت','قفل')\n",
    "            in_user=in_user.replace('سكر','قفل')\n",
    "            in_user=in_user.replace('صاكه','قفل')\n",
    "            in_user=in_user.replace('انعدمت','خدشت')\n",
    "\n",
    "            in_user=in_user.replace('ورع','طفل')\n",
    "            in_user=in_user.replace('بزر','طفل')\n",
    "            in_user=in_user.replace('بيبي','طفل')\n",
    "\n",
    "            in_user=in_user.replace('قسائمي','مخالفاتي')\n",
    "\n",
    "            arb_stopwords = ['الحل','مانوع','ارى','من','وين','معرف','مطفأة','طارئة',' الحالة','جدا','مهم','طواريء','بلاغ','استطع','لم','لان','ولا','الفاعل','الشكوى','اريد','علي','عن','صار','ابغى','ايش','بها','هي','هو','على','وش','ان','لي','هل','فيه','فيها','ماهي','هي','ما','في','هناك','واودّ','و','تم','قام','عليه','عليهم','لقيت','الظاهر']\n",
    "            in_user=normalize_alef_ar(in_user)\n",
    "            in_user=normalize_alef_maksura_ar(in_user)\n",
    "            y=in_user.split()\n",
    "            print(y)\n",
    "            n=[]\n",
    "            for i in range(0, len(y)):\n",
    "                if(y[i] not in arb_stopwords):\n",
    "                    n.append(y[i])\n",
    "            print(y)\n",
    "            print(n)\n",
    "\n",
    "            m=[]\n",
    "\n",
    "            for i in range(0, len(n)):\n",
    "                m.append(''.join(c for c in n[i] if not ud.category(c).startswith('P')))\n",
    "\n",
    "            x = \" \".join(m)\n",
    "            stemmer=FarasaStemmer()\n",
    "            x=stemmer.stem(x)\n",
    "            print(x)\n",
    "            topics= my_model.transform(x)\n",
    "            print(topics)\n",
    "\n",
    "            if(topics[0]==[7]):\n",
    "                    txt.insert(END, \"\\n\" + \"Bot -> وعليكم السلام ورحمة الله و بركاته؛ اهلا ... كيف استطيع خدمتك؟\")\n",
    "\n",
    "            elif(topics[0]==[0]):\n",
    "                txt.insert(END, \"\\n\" + \"Bot -> الاستعلام عن مخالفه :الرجاء تزويدنا برقم اللوحه من اليمين الى اليسار\")\n",
    "                #button.configure(command= violation)\n",
    "\n",
    "                tk.Button(root, text=\"استعلم\", command=violation).grid(row=1, column=2)\n",
    "\n",
    "\n",
    "            elif(topics[0]==[1] or topics[0]==[6]):\n",
    "                txt.insert(END, \"\\n\" + \"Bot ->التبليغ عن حادث :الرجاء تزويدنا برقم اللوحه من اليمين الى اليسار \")\n",
    "                #button.configure(command= scratch)\n",
    "\n",
    "                tk.Button(root, text=\"التبليغ\", command=scratch).grid(row=1, column=2)\n",
    "\n",
    "            elif(topics[0]==[-1] or topics[0]==[3] or topics[0]==[4]):\n",
    "                txt.insert(END, \"\\n\" + \"Bot -> التبليغ عن اطفال لوحدهم :الرجاء تزويدنا برقم اللوحه من اليمين الى اليسار\")\n",
    "                #button.configure(command= alone_child)\n",
    "                tk.Button(root, text=\"التبليغ\", command=alone_child).grid(row=1, column=2)\n",
    "\n",
    "   \n",
    "\n",
    "            elif(topics[0]==[2] or topics[0]==[5]):\n",
    "                txt.insert(END, \"\\n\" + \"Bot -> التبليغ عن سيارة مقفلة الطريق :الرجاء تزويدنا برقم اللوحه من اليمين الى اليسار\")\n",
    "                #button.configure(command= close_road)\n",
    "                tk.Button(root, text=\"التبليغ\", command=close_road).grid(row=1, column=2)\n",
    "\n",
    "            else:\n",
    "                print('Bot -> لا استطيع فهمك.\\n الرجاء ادخال الشكوى الصحيحه:\\n 1- استفسار عن مخالفه.\\n 2- تبليغ عن خدش في السياره.\\n 3- اطفال مغلق عليهم في سيارة.\\n 4- سيارة موقفة خلف السيارة.')\n",
    "            e.delete(0, 'end')\n",
    "\n",
    "\n",
    "\n",
    "txt = Text(root)\n",
    "txt.grid(row=0, column=0, columnspan=3)\n",
    "e = Entry(root, width=100)\n",
    "e.grid(row=1, column=0)\n",
    "button=tk.Button(root, text=\"ارسال\", command=send)\n",
    "button.grid(row=1, column=1)\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNbfAcQSByFv"
   },
   "source": [
    "If you use this notebook, please cite our paper :)\n",
    "\n",
    "```\n",
    "Abeer Abuzayed and Hend Al-Khalifa. BERT for Arabic Topic Modeling: An Experimental Study on BERTopic Technique. Arabic Computational Linguistics, Procedia Computer Science, Elsevier, (in press).\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP and Unsupervised Learning.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
